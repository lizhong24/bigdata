一、Spark概述
	官网：http://spark.apache.org/
	Apache Spark™是用于大规模数据处理的统一分析引擎。
	为大数据处理而设计的快速通用的计算引擎。
	
	Spark加州大学伯克利分校AMP实验室。不同于mapreduce的是一个Spark任务的中间结果保存到内存中。
	空间换时间。
	Spark启用的是内存分布式数据集。
	用scala语言实现，与spark紧密继承。用scala可以轻松的处理分布式数据集。
	Spark并不是为了替代hadoop，而为了补充hadoop。
	Spark并没有存储。可以集成HDFS。
	
二、Spark特点
	1）速度快
	与mr对比，磁盘运行的话10倍以上。
	内存运行的话，100倍以上。
	
	2）便于使用
	支持java/scala/python/R
	
	3)通用
	不仅支持批处理（SparkSQL）
	而且支持流处理（SparkStreaming）
	
	4)兼容
	兼容其它组件
	Spark实现了Standalone作为内置的资源管理和调度框架。hdfs/yarn。

三、Spark安装部署
	主节点：Master （192.168.146.150）
	从节点：Worker （192.168.146.151、192.168.146.152）
	
	1、准备工作	
	（1）关闭防火墙
        firewall-cmd --state 查看防火墙状态
        systemctl stop firewalld.service 关闭防火墙
        systemctl disable firewalld.service 禁止开机启动
        
    （2）远程连接（CRT）
        
        
    （3）永久设置主机名
        vi /etc/hostname
		三台机器hostname分别为spark-01、spark-02、spark-03
        注意：要reboot重启生效
        
    （4）配置映射文件
        vi /etc/hosts
        
        #127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
        #::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
        192.168.146.150 spark-01
        192.168.146.151 spark-02
        192.168.146.152 spark-03
		
	（5）配置ssh免密码登录
	    ssh-keygen  生成密钥对
        ssh-copy-id spark-01
        ssh-copy-id spark-02
        ssh-copy-id spark-03
	
	2、安装jdk(scala依赖jvm)
	（1）创建spark安装的目录
		cd /root
		上传tar包到/root目录下        
        
    （2）解压tar包
		cd /root
		mkdir sk	
        tar -zxvf jdk-8u144-linux-x64.tar.gz -C /root/sk	
    
    （3）配置环境变量
        vi /etc/profile 
        
        export JAVA_HOME=/root/sk/jdk1.8.0_144
        export PATH=$PATH:$JAVA_HOME/bin
        
        source /etc/profile  加载环境变量
        
    （4）发送到其它机器(其他机器的/root下要先创建sk目录)
		cd /root/sk
		scp -r jdk1.8.0_144/ root@spark-02:$PWD
		scp -r jdk1.8.0_144/ root@spark-03:$PWD
		
        scp -r /etc/profile spark-02:/etc
        scp -r /etc/profile spark-03:/etc
        
        注意：加载环境变量 source /etc/profile
	
	3、安装Spark集群	
	（1）上传tar包到/root目录下    
	
	（2）解压
		cd /root
		tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz -C sk/
		
	（3）修改配置文件
		cd /root/sk/spark-2.2.0-bin-hadoop2.7/conf
		mv spark-env.sh.template spark-env.sh
		vi spark-env.sh

		export JAVA_HOME=/root/sk/jdk1.8.0_144
		export SPARK_MASTER_HOST=spark-01
		export SPARK_MASTER_PORT=7077 
	
	（4）slaves 加入从节点
		cd /root/sk/spark-2.2.0-bin-hadoop2.7/conf
		mv slaves.template slaves
		vi slaves
		
		spark-02
		spark-03
		
	（5）分发到其他机器
		cd /root/sk
		scp -r spark-2.2.0-bin-hadoop2.7/ root@spark-02:$PWD
		scp -r spark-2.2.0-bin-hadoop2.7/ root@spark-03:$PWD
		
	（6）启动集群
		cd /root/sk/spark-2.2.0-bin-hadoop2.7
		sbin/start-all.sh
		
		浏览器访问http://spark-01:8080/即可看到UI界面
		
	（7）启动命令行模式
		cd /root/sk/spark-2.2.0-bin-hadoop2.7/bin
		./spark-shell 
		
		sc.textFile("/root/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortBy((_,1)).collect

	