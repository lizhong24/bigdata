一、Spark概述
	官网：http://spark.apache.org/
	Apache Spark™是用于大规模数据处理的统一分析引擎。
	为大数据处理而设计的快速通用的计算引擎。
	
	Spark加州大学伯克利分校AMP实验室。不同于mapreduce的是一个Spark任务的中间结果保存到内存中。
	空间换时间。
	Spark启用的是内存分布式数据集。
	用scala语言实现，与spark紧密继承。用scala可以轻松的处理分布式数据集。
	Spark并不是为了替代hadoop，而为了补充hadoop。
	Spark并没有存储。可以集成HDFS。
	
二、Spark特点
	1）速度快
	与mr对比，磁盘运行的话10倍以上。
	内存运行的话，100倍以上。
	
	2）便于使用
	支持java/scala/python/R
	
	3)通用
	不仅支持批处理（SparkSQL）
	而且支持流处理（SparkStreaming）
	
	4)兼容
	兼容其它组件
	Spark实现了Standalone作为内置的资源管理和调度框架。hdfs/yarn。

三、Spark安装部署
	主节点：Master （192.168.146.150）
	从节点：Worker （192.168.146.151、192.168.146.152）
	
	1、准备工作	
	（1）关闭防火墙
        firewall-cmd --state 查看防火墙状态
        systemctl stop firewalld.service 关闭防火墙
        systemctl disable firewalld.service 禁止开机启动
        
    （2）远程连接（CRT）     
        
    （3）永久设置主机名
        vi /etc/hostname
		三台机器hostname分别为spark-01、spark-02、spark-03
        注意：要reboot重启生效
        
    （4）配置映射文件
        vi /etc/hosts
        
        #127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
        #::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
        192.168.146.150 spark-01
        192.168.146.151 spark-02
        192.168.146.152 spark-03
		
	（5）配置ssh免密码登录
	    ssh-keygen  生成密钥对
        ssh-copy-id spark-01
        ssh-copy-id spark-02
        ssh-copy-id spark-03
	
	2、安装jdk(scala依赖jvm)
	（1）创建spark安装的目录
		cd /root
		上传tar包到/root目录下        
        
    （2）解压tar包
		cd /root
		mkdir sk	
        tar -zxvf jdk-8u144-linux-x64.tar.gz -C /root/sk	
    
    （3）配置环境变量
        vi /etc/profile 
        
        export JAVA_HOME=/root/sk/jdk1.8.0_144
        export PATH=$PATH:$JAVA_HOME/bin
        
        source /etc/profile  加载环境变量
        
    （4）发送到其它机器(其他机器的/root下要先创建sk目录)
		cd /root/sk
		scp -r jdk1.8.0_144/ root@spark-02:$PWD
		scp -r jdk1.8.0_144/ root@spark-03:$PWD
		
        scp -r /etc/profile spark-02:/etc
        scp -r /etc/profile spark-03:/etc
        
        注意：加载环境变量 source /etc/profile
	
	3、安装Spark集群	
	（1）上传tar包到/root目录下    
	
	（2）解压
		cd /root
		tar -zxvf spark-2.2.0-bin-hadoop2.7.tgz -C sk/
		
	（3）修改配置文件
		cd /root/sk/spark-2.2.0-bin-hadoop2.7/conf
		mv spark-env.sh.template spark-env.sh
		vi spark-env.sh

		export JAVA_HOME=/root/sk/jdk1.8.0_144
		export SPARK_MASTER_HOST=spark-01
		export SPARK_MASTER_PORT=7077 
	
	（4）slaves 加入从节点
		cd /root/sk/spark-2.2.0-bin-hadoop2.7/conf
		mv slaves.template slaves
		vi slaves
		
		spark-02
		spark-03
		
	（5）分发到其他机器
		cd /root/sk
		scp -r spark-2.2.0-bin-hadoop2.7/ root@spark-02:$PWD
		scp -r spark-2.2.0-bin-hadoop2.7/ root@spark-03:$PWD
		
	（6）启动集群
		cd /root/sk/spark-2.2.0-bin-hadoop2.7
		sbin/start-all.sh
		
		浏览器访问http://spark-01:8080/即可看到UI界面
		
	（7）启动命令行模式
		cd /root/sk/spark-2.2.0-bin-hadoop2.7/bin
		./spark-shell 
		
		sc.textFile("/root/words.txt").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).sortBy((_,1)).collect

四、启动spark­shell
	cd /root/sk/spark-2.2.0-bin-hadoop2.7/
	本地模式：bin/spark-shell
	
	集群启动：bin/spark-shell --master spark://spark-01:7077 --total-executor-cores 2 --executor-memory 512mb
	
	提交运行jar：bin/spark-submit --master spark://spark-01:7077 --class SparkWordCount /root/SparkWC-1.0-SNAPSHOT.jar hdfs://192.168.146.111:9000/words.txt hdfs://192.168.146.111:9000/sparkwc/out

五、spark集群角色
	Yarn						 Spark			 作用
	ResourceManager				 Master			 管理子节点
	NodeManager					 Worker			 管理当前节点
	YarnChild					 Executor 		 处理计算任务
	Client+ApplicationMaster	 SparkSubmit	 提交计算任务

六、sparkRDD算子
	RDD(Resilient Distributed DataSet)是分布式数据集。RDD是Spark最基本的数据的抽象。
	scala中的集合。RDD相当于一个不可变、可分区、里面的元素可以并行计算的集合。
	
	RDD特点：
	具有数据流模型的特点
	自动容错
	位置感知调度
	可伸缩性
	RDD允许用户在执行多个查询时将工作集缓存在内存中，可以重用工作集，大大的提升了查询速度。
	
	RDD类型分为：
	1）Transformation
	转换
	2）Action
	动作

七、RDD创建
	RDD分为两种类型：
	1）Transformation（lazy-》懒加载）
	2）Action（触发任务）
	
	scala> sc.textFile("/root/words.txt")
	res8: org.apache.spark.rdd.RDD[String] = /root/words.txt MapPartitionsRDD[28] at textFile at <console>:25

	scala> sc.textFile("/root/words.txt").flatMap(_.split(" "))
	res9: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[31] at flatMap at <console>:25

	scala> res8
	res10: org.apache.spark.rdd.RDD[String] = /root/words.txt MapPartitionsRDD[28] at textFile at <console>:25

	scala> res8.flatMap(_.split(" "))
	res11: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[32] at flatMap at <console>:27

	scala> res8.flatMap(_.split(" ")).map((_,1))
	res12: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[34] at map at <console>:27

	scala> res8.flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_)
	res13: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[37] at reduceByKey at <console>:27

	scala> res13.collect
	res14: Array[(String, Int)] = Array((is,1), (love,2), (capital,1), (Beijing,2), (China,2), (I,2), (of,1), (the,1))

	scala> val rdd1 = sc.textFile("/root/words.txt")
	rdd1: org.apache.spark.rdd.RDD[String] = /root/words.txt MapPartitionsRDD[39] at textFile at <console>:24

	scala> rdd1.count
	res15: Long = 3

	scala> 
	
	
	scala> val list = List(1,3,5,7)
	list: List[Int] = List(1, 3, 5, 7)

	scala> list.map(_ * 100)
	res16: List[Int] = List(100, 300, 500, 700)

	scala> val rdd1 = sc.parallelize(list)
	rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[40] at parallelize at <console>:26

	scala> rdd1.map(_ * 100)
	res17: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[41] at map at <console>:29

	scala> res17.collect
	res18: Array[Int] = Array(100, 300, 500, 700)

	scala> val rdd2 = sc.makeRDD(list)
	rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[42] at makeRDD at <console>:26

	scala> val rdd1 = sc.parallelize(List(1,2,3,4,5),3)
	rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[43] at parallelize at <console>:24

	scala> val rdd2 = rdd1.map(_ * 1000)
	rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[44] at map at <console>:26

	scala> rdd2.collect
	res19: Array[Int] = Array(1000, 2000, 3000, 4000, 5000)

	scala> 

	
八、常用Transformation
	map(func)
	flatMap(func)
	sortby
	reduceByKey
	
	-》filter
	过滤
	-》union
	并集
	-》groupByKey
	分组
	-》intersection
	交集
	-》join
	关联
	-》leftOuterJoin
	左连接
	保留左侧RDD，右侧如果join上保留没join上None
	-》rightOuterJoin
	右连接
	-》cartesian
	笛卡尔积
	
	Transformation特点：
	1）生成新的RDD
	2）lazy懒加载 等待处理
	3）并不会存储真正的数据，记录了转换关系

	
	scala> val rdd1 = sc.parallelize(List(1,2,3,4,5))
	rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[48] at parallelize at <console>:24

	scala> rdd1.filter(_ % 2 == 0)
	res24: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[49] at filter at <console>:27

	scala> res24.collect
	res25: Array[Int] = Array(2, 4)

	scala> val rdd2 = sc.parallelize(List(1,2,3,4,5,6,7))
	rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[50] at parallelize at <console>:24

	scala> rdd1.union(rdd2)
	res26: org.apache.spark.rdd.RDD[Int] = UnionRDD[51] at union at <console>:29

	scala> res26.collect
	res27: Array[Int] = Array(1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7)

	scala> rdd1 union rdd2
	res28: org.apache.spark.rdd.RDD[Int] = UnionRDD[52] at union at <console>:29

	scala> res28.collect
	res29: Array[Int] = Array(1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 6, 7)

	scala> val rdd3 = sc.parallelize(List(("Tom",18),("John",16),("Tom",20),("Mary",17),("John",23))
		 | )
	rdd3: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[53] at parallelize at <console>:24

	scala> rdd3.groupByKey
	res30: org.apache.spark.rdd.RDD[(String, Iterable[Int])] = ShuffledRDD[54] at groupByKey at <console>:27

	scala> res30.collect
	res31: Array[(String, Iterable[Int])] = Array((John,CompactBuffer(16, 23)), (Tom,CompactBuffer(18, 20)), (Mary,CompactBuffer(17)))

	scala> 

	
	scala> val rdd1 = sc.parallelize(List(1,2,3,4,5))
	rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[55] at parallelize at <console>:24

	scala> val rdd2 = sc.parallelize(List(1,2,3,4,5,6,7))
	rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[56] at parallelize at <console>:24

	scala> rdd1.intersection(rdd2)
	res33: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[62] at intersection at <console>:29

	scala> res33.collect
	res34: Array[Int] = Array(3, 4, 1, 5, 2)

	scala> res33.collect
	res35: Array[Int] = Array(3, 4, 1, 5, 2)

	scala> res33.collect
	res36: Array[Int] = Array(3, 4, 1, 5, 2)

	scala> res33.collect
	res37: Array[Int] = Array(3, 4, 1, 5, 2)

	scala> 
	
	
	scala> val rdd1 = sc.parallelize(List(("Tom",18),("John",16),("Mary",21)))
	rdd1: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[65] at parallelize at <console>:24

	scala> val rdd2 = sc.parallelize(List(("Tom",28),("John",26),("Cat",17)))
	rdd2: org.apache.spark.rdd.RDD[(String, Int)] = ParallelCollectionRDD[66] at parallelize at <console>:24

	scala> rdd1.join(rdd2)
	res39: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[69] at join at <console>:29

	scala> res39.collect
	res40: Array[(String, (Int, Int))] = Array((John,(16,26)), (Tom,(18,28)))

	scala> rdd1.leftOuterJoin(rdd2)
	res41: org.apache.spark.rdd.RDD[(String, (Int, Option[Int]))] = MapPartitionsRDD[72] at leftOuterJoin at <console>:29

	scala> res41.collect
	res42: Array[(String, (Int, Option[Int]))] = Array((John,(16,Some(26))), (Tom,(18,Some(28))), (Mary,(21,None)))

	scala> rdd1.rightOuterJoin(rdd2).collect
	res43: Array[(String, (Option[Int], Int))] = Array((John,(Some(16),26)), (Tom,(Some(18),28)), (Cat,(None,17)))

	scala> val rdd1 = sc.parallelize(List("Tom","Mary"))
	rdd1: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[76] at parallelize at <console>:24

	scala> val rdd2 = sc.parallelize(List("John","Joe"))
	rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[77] at parallelize at <console>:24

	scala> rdd1.cartesian(rdd2)
	res45: org.apache.spark.rdd.RDD[(String, String)] = CartesianRDD[78] at cartesian at <console>:29

	scala> res45.collect
	res46: Array[(String, String)] = Array((Tom,John), (Tom,Joe), (Mary,John), (Mary,Joe))

	scala> 

	


